{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS IS FOR THE AUDIO EMOTION RECOGNITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in Audio2\\emotion.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported Audio2\\chunk_1.mp3\n",
      "Exported Audio2\\chunk_2.mp3\n",
      "Exported Audio2\\chunk_3.mp3\n",
      "Deleted original audio file at Audio2\\emotion.mp3\n",
      "Imported the model named <keras.engine.sequential.Sequential object at 0x000002A5D3EF8A60> \n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "  predictedvalues\n",
      "0         fearful\n",
      "1           angry\n",
      "2           angry\n",
      "  percentage  proportion\n",
      "0      angry   66.666667\n",
      "1    fearful   33.333333\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from moviepy.editor import VideoFileClip\n",
    "from pydub import AudioSegment\n",
    "from math import ceil\n",
    "import os  # Ensure os is imported\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "video_path = \"Video1\\Angry_video.mp4\"  # Change this to the path of your video file\n",
    "audio_path = \"Audio2\\emotion.mp3\"  # Output path for the extracted audio\n",
    "\n",
    "# Load the video file and extract the audio\n",
    "video_clip = VideoFileClip(video_path)\n",
    "audio_clip = video_clip.audio\n",
    "audio_clip.write_audiofile(audio_path)\n",
    "\n",
    "audio_clip.close()\n",
    "video_clip.close()\n",
    "\n",
    "# Load the extracted audio\n",
    "audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "# Define the length of each chunk in milliseconds\n",
    "chunk_length_ms = 3000\n",
    "\n",
    "# Calculate the number of chunks needed\n",
    "num_chunks = ceil(len(audio) / chunk_length_ms)\n",
    "\n",
    "# Split the audio and save each chunk\n",
    "for i in range(num_chunks):\n",
    "    start_time = i * chunk_length_ms\n",
    "    end_time = min((i + 1) * chunk_length_ms, len(audio))\n",
    "    chunk = audio[start_time:end_time]\n",
    "    chunk_name = f'Audio2\\chunk_{i+1}.mp3'  # Naming each chunk\n",
    "    chunk.export(chunk_name, format=\"mp3\")\n",
    "    print(f'Exported {chunk_name}')\n",
    "\n",
    "# After exporting all chunks, delete the original audio file\n",
    "os.remove(audio_path)\n",
    "print(f'Deleted original audio file at {audio_path}')\n",
    "\n",
    "# Directory where the chunks are saved\n",
    "chunks_dir = 'Audio2'\n",
    "audio_files = [f for f in os.listdir(chunks_dir) if f.endswith('.mp3')]\n",
    "audio_files.sort()  # Optional, to process the files in a sorted order\n",
    "\n",
    "df = pd.DataFrame(columns=['feature'])\n",
    "bookmark = 0\n",
    "\n",
    "for index, filename in enumerate(audio_files):\n",
    "    # Adjust the condition according to your naming convention, if needed\n",
    "    file_path = os.path.join(chunks_dir, filename)\n",
    "    X, sample_rate = librosa.load(file_path, res_type='kaiser_fast', duration=2.5, sr=22050*2, offset=0.5)\n",
    "    sample_rate = np.array(sample_rate)\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\n",
    "    feature = mfccs\n",
    "    df.loc[bookmark] = [feature]\n",
    "    bookmark += 1\n",
    "\n",
    "# Now, df contains the features extracted from each audio chunk\n",
    "#print(df)\n",
    "df = pd.DataFrame(df['feature'].values.tolist())\n",
    "df[:]\n",
    "df.fillna('0')\n",
    "\n",
    "model=load_model('Voice/Voice-Emotion-Detector/saved_models/Emotion_Voice_Detection_Model.h5')   \n",
    "print('Imported the model named %s ' % model)\n",
    "\n",
    "labels = ['female_angry', 'female_calm', 'female_fearful', 'female_happy', 'female_sad', 'male_angry', 'male_calm', 'male_fearful', 'male_happy', 'male_sad']\n",
    "\n",
    "lb = LabelEncoder()\n",
    "\n",
    "# Fit the LabelEncoder instance to your known labels\n",
    "lb.fit(labels)\n",
    "\n",
    "x_traincnn =np.expand_dims(df, axis=2)\n",
    "\n",
    "preds = model.predict(x_traincnn, batch_size=1, verbose=1)\n",
    "preds1=preds.argmax(axis=1)\n",
    "abc = preds1.astype(int).flatten()\n",
    "predictions = (lb.inverse_transform((abc)))\n",
    "preddf = pd.DataFrame({'predictedvalues': predictions})\n",
    "preddf[:]\n",
    "\n",
    "# Assuming preddf is your DataFrame\n",
    "# Remove 'male_' and 'female_' prefixes from the 'predictedvalues' column\n",
    "preddf['predictedvalues'] = preddf['predictedvalues'].str.replace('female_', '').str.replace('male_', '')\n",
    "\n",
    "print(preddf)\n",
    "# Calculate the percentage of each emotion\n",
    "emotion_counts = preddf['predictedvalues'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Convert the emotion_counts to a DataFrame for nicer formatting, if desired\n",
    "emotion_percentage_df_audio_sound = emotion_counts.reset_index().rename(columns={'index': 'emotion', 'predictedvalues': 'percentage'})\n",
    "\n",
    "# Display the emotion percentage DataFrame\n",
    "print(emotion_percentage_df_audio_sound)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the text analysis to find the emotions in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch is using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription saved to Audio2\\chunk_1.txt\n",
      "Transcription saved to Audio2\\chunk_2.txt\n",
      "Transcription saved to Audio2\\chunk_3.txt\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "\n",
    "# Check if CUDA is available for GPU usage, else use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"PyTorch is using device: {device}\")\n",
    "\n",
    "# Load the pretrained Wav2Vec2 model and processor\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\").to(device)\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# Directory where the chunks are saved\n",
    "chunks_dir = 'Audio2'  # Update this path to your chunks directory\n",
    "audio_chunks = os.listdir(chunks_dir)\n",
    "\n",
    "# Function to transcribe a single audio file\n",
    "def transcribe(audio_path):\n",
    "    # Load the audio file using librosa\n",
    "    speech, sr = librosa.load(audio_path, sr=16000)  # Resample to 16000 Hz\n",
    "    \n",
    "    # Process the speech file\n",
    "    input_values = processor(speech, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
    "    input_values = input_values.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.batch_decode(predicted_ids)\n",
    "    return transcription\n",
    "\n",
    "# Iterate over audio chunks and save transcriptions to text files\n",
    "for chunk in audio_chunks:\n",
    "    if chunk.endswith('.mp3'):  # Make sure to process only audio files\n",
    "        audio_path = os.path.join(chunks_dir, chunk)\n",
    "        transcription = transcribe(audio_path)\n",
    "        \n",
    "        # Saving the transcription to a text file\n",
    "        text_filename = chunk.replace('.mp3', '.txt')\n",
    "        text_path = os.path.join(chunks_dir, text_filename)  # Saving text files in the same directory as chunks\n",
    "        with open(text_path, 'w') as text_file:\n",
    "            text_file.write(transcription[0])  # Assuming one transcription per audio chunk\n",
    "\n",
    "        print(f\"Transcription saved to {text_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Emotion 1 Prob Emotion 1 Emotion 2 Prob Emotion 2  Emotion 3 Prob Emotion 3\n",
      "0   neutral         0.7595     anger         0.1270  annoyance         0.0659\n",
      "1   neutral         0.9962  approval         0.0006  annoyance         0.0006\n",
      "2   neutral         0.9301     anger         0.0355  annoyance         0.0094\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Function to read text from a file\n",
    "def read_text_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read().strip()\n",
    "\n",
    "# Function to classify text\n",
    "def classify_emotion(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "    return probabilities\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
    "\n",
    "# Path to your directory containing .txt files\n",
    "text_files_directory = 'Audio2/'\n",
    "text_files = [f for f in os.listdir(text_files_directory) if f.endswith('.txt')]\n",
    "\n",
    "labels = model.config.id2label\n",
    "\n",
    "# Initialize a list to collect dictionaries\n",
    "data = []\n",
    "\n",
    "for text_file in text_files:\n",
    "    file_path = os.path.join(text_files_directory, text_file)\n",
    "    text = read_text_from_file(file_path)\n",
    "    \n",
    "    probabilities = classify_emotion(text, tokenizer, model)\n",
    "    top_probs, top_lbls = torch.topk(probabilities, 3, dim=-1)\n",
    "    \n",
    "    # Append a new dictionary to the list for each file\n",
    "    data.append({\n",
    "        'Emotion 1': labels[top_lbls[0][0].item()], 'Prob Emotion 1': f\"{top_probs[0][0].item():.4f}\",\n",
    "        'Emotion 2': labels[top_lbls[0][1].item()], 'Prob Emotion 2': f\"{top_probs[0][1].item():.4f}\",\n",
    "        'Emotion 3': labels[top_lbls[0][2].item()], 'Prob Emotion 3': f\"{top_probs[0][2].item():.4f}\",\n",
    "    })\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file\n",
    "df.to_csv('emotion_predictions_from_audio_text.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code to analyze the audio and generate a text file of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    Wav2Vec2ForCTC, Wav2Vec2Processor,\n",
    "    AutoTokenizer, AutoModelForSequenceClassification\n",
    ")\n",
    "import librosa\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Check device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"PyTorch is using device: {device}\")\n",
    "\n",
    "# Load Wav2Vec2 model and processor for transcription\n",
    "wav2vec2_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\").to(device)\n",
    "wav2vec2_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# Load RoBERTa model and tokenizer for emotion classification\n",
    "emotion_tokenizer = AutoTokenizer.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
    "emotion_model = AutoModelForSequenceClassification.from_pretrained(\"SamLowe/roberta-base-go_emotions\").to(device)\n",
    "\n",
    "# Define labels\n",
    "labels = emotion_model.config.id2label\n",
    "\n",
    "# Function to transcribe audio\n",
    "def transcribe(audio_path):\n",
    "    speech, sr = librosa.load(audio_path, sr=16000)\n",
    "    input_values = wav2vec2_processor(speech, return_tensors=\"pt\", sampling_rate=16000).input_values.to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = wav2vec2_model(input_values).logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = wav2vec2_processor.batch_decode(predicted_ids)\n",
    "    return transcription[0]\n",
    "\n",
    "# Function to classify text\n",
    "def classify_emotion(text):\n",
    "    inputs = emotion_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = emotion_model(**inputs).logits\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "    top_probs, top_lbls = torch.topk(probabilities, 3, dim=-1)\n",
    "    return [(labels[top_lbls[0][i].item()], top_probs[0][i].item()) for i in range(3)]\n",
    "\n",
    "# Directory with audio chunks\n",
    "chunks_dir = 'Audio2'  # Adjust as necessary\n",
    "audio_chunks = [f for f in os.listdir(chunks_dir) if f.endswith('.mp3')]\n",
    "\n",
    "# Initialize list for DataFrame\n",
    "data = []\n",
    "\n",
    "\n",
    "# Specify the directory where you want to save the text files\n",
    "text_files_directory = 'Audio2'\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(text_files_directory, exist_ok=True)\n",
    "\n",
    "# Process each chunk\n",
    "for chunk in audio_chunks:\n",
    "    audio_path = os.path.join(chunks_dir, chunk)\n",
    "    transcription = transcribe(audio_path)\n",
    "    \n",
    "    # Define the path for the new text file\n",
    "    text_filename = chunk.replace('.mp3', '.txt')\n",
    "    text_file_path = os.path.join(text_files_directory, text_filename)\n",
    "    \n",
    "    # Save the transcription to the text file\n",
    "    with open(text_file_path, 'w', encoding='utf-8') as text_file:\n",
    "        text_file.write(transcription)\n",
    "    \n",
    "    print(f\"Transcription saved to {text_file_path}\")\n",
    "\n",
    "    # Now proceed with emotion classification\n",
    "    emotions = classify_emotion(transcription)\n",
    "    \n",
    "    # Prepare data for DataFrame\n",
    "    row = {}  # Using text_filename to refer to the saved file\n",
    "    for i, emo in enumerate(emotions):\n",
    "        row[f'Emotion {i+1}'] = emo[0]\n",
    "        row[f'Prob Emotion {i+1}'] = f\"{emo[1]:.4f}\"\n",
    "    \n",
    "    data.append(row)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display DataFrame\n",
    "print(df)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('emotion_predictions_from_audio_text.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from paz.applications import HaarCascadeFrontalFace, MiniXceptionFER\n",
    "import paz.processors as pr\n",
    "\n",
    "class EmotionDetector(pr.Processor):\n",
    "    def __init__(self):\n",
    "        super(EmotionDetector, self).__init__()\n",
    "        self.detect = HaarCascadeFrontalFace(draw=False)\n",
    "        self.crop = pr.CropBoxes2D()\n",
    "        self.classify = MiniXceptionFER()\n",
    "        self.draw = pr.DrawBoxes2D(self.classify.class_names)\n",
    "\n",
    "    def call(self, image):\n",
    "        boxes2D = self.detect(image)['boxes2D']\n",
    "        emotions = []\n",
    "        for cropped_image, box2D in zip(self.crop(image, boxes2D), boxes2D):\n",
    "            emotion = self.classify(cropped_image)['class_name']\n",
    "            emotions.append(emotion)\n",
    "            box2D.class_name = emotion\n",
    "        image_with_boxes = self.draw(image, boxes2D)\n",
    "        return image_with_boxes, emotions\n",
    "\n",
    "# Initialize the emotion detector\n",
    "detect = EmotionDetector()\n",
    "\n",
    "video_chunks_dir = 'Video2'\n",
    "images_dir = os.path.join(video_chunks_dir, 'images')\n",
    "os.makedirs(images_dir, exist_ok=True)\n",
    "\n",
    "# Process each video chunk\n",
    "for chunk_filename in os.listdir(video_chunks_dir):\n",
    "    if chunk_filename.endswith('.mp4'):\n",
    "        chunk_path = os.path.join(video_chunks_dir, chunk_filename)\n",
    "        # Extract frames and apply the EmotionDetector\n",
    "        cap = cv2.VideoCapture(chunk_path)\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        emotions_all_frames = []\n",
    "\n",
    "        for _ in range(frame_count):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            _, emotions = detect(frame)\n",
    "            emotions_all_frames.extend(emotions)\n",
    "\n",
    "            # Optionally save frame with detected emotions drawn\n",
    "            # frame_save_path = os.path.join(images_dir, f\"frame_{frame_index}_{chunk_filename}.jpg\")\n",
    "            # cv2.imwrite(frame_save_path, cv2.cvtColor(processed_frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "        # Calculate the average emotion for the video chunk\n",
    "        # This part requires defining how you quantify and average emotions\n",
    "        # Example: Count occurrences of each emotion and find the most frequent\n",
    "\n",
    "        cap.release()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Athena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
